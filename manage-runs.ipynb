{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/track-and-monitor-experiments/manage-runs/manage-runs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manage runs\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Start, monitor and complete a run](#Start,-monitor-and-complete-a-run)\n",
    "1. [Add properties and tags](#Add-properties-and-tags)\n",
    "1. [Query properties and tags](#Query-properties-and-tags)\n",
    "1. [Start and query child runs](#Start-and-query-child-runs)\n",
    "1. [Cancel or fail runs](#Cancel-or-fail-runs)\n",
    "1. [Reproduce a run](#Reproduce-a-run)\n",
    "1. [Next steps](#Next-steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "When you're building enterprise-grade machine learning models, it is important to track, organize, monitor and reproduce your training runs. For example, you might want to trace the lineage behind a model deployed to production, and re-run the training experiment to troubleshoot issues. \n",
    "\n",
    "This notebooks shows examples how to use Azure Machine Learning services to manage your training runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set.  Otherwise, go through the [configuration](../../../configuration.ipynb) Notebook first if you haven't already to establish your connection to the AzureML Workspace. Also, if you're new to Azure ML, we recommend that you go through [the tutorial](https://docs.microsoft.com/en-us/azure/machine-learning/service/tutorial-train-models-with-aml) first to learn the basic concepts.\n",
    "\n",
    "Let's first import required packages, check Azure ML SDK version, connect to your workspace and create an Experiment to hold the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.18.0\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Run\n",
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "print(azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(workspace=ws, name=\"explore-runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start, monitor and complete a run\n",
    "\n",
    "A run is an unit of execution, typically to train a model, but for other purposes as well, such as loading or transforming data. Runs are tracked by Azure ML service, and can be instrumented with metrics and artifact logging.\n",
    "\n",
    "A simplest way to start a run in your interactive Python session is to call *Experiment.start_logging* method. You can then log metrics from within the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running\n"
     ]
    }
   ],
   "source": [
    "notebook_run = exp.start_logging()\n",
    "\n",
    "notebook_run.log(name=\"message\", value=\"Hello from run!\")\n",
    "\n",
    "print(notebook_run.get_status())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use *get_status method* to get the status of the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running\n"
     ]
    }
   ],
   "source": [
    "print(notebook_run.get_status())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, you can simply enter the run to get a link to Azure Portal details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>explore-runs</td><td>ccd7d9a0-8fff-4591-aee8-576e5a778e22</td><td></td><td>Running</td><td><a href=\"https://ml.azure.com/experiments/explore-runs/runs/ccd7d9a0-8fff-4591-aee8-576e5a778e22?wsid=/subscriptions/888519c8-2387-461a-aff3-b31b86e2438e/resourcegroups/aml-quickstarts-126067/workspaces/quick-starts-ws-126067\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.run.Run?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: explore-runs,\n",
       "Id: ccd7d9a0-8fff-4591-aee8-576e5a778e22,\n",
       "Type: None,\n",
       "Status: Running)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notebook_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method *get_details* gives you more details on the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'runId': 'ccd7d9a0-8fff-4591-aee8-576e5a778e22',\n",
       " 'target': 'local',\n",
       " 'status': 'Running',\n",
       " 'startTimeUtc': '2020-11-14T07:48:35.052863Z',\n",
       " 'properties': {'ContentSnapshotId': '5888d6a8-3fb3-4558-9bd7-1b3d2e91638e'},\n",
       " 'inputDatasets': [],\n",
       " 'outputDatasets': [],\n",
       " 'logFiles': {}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notebook_run.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use *complete* method to end the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "notebook_run.complete()\n",
    "print(notebook_run.get_status())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use Python's *with...as* pattern. The run will automatically complete when moving out of scope. This way you don't need to manually complete the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is it still running? Running\n",
      "Has it completed? Completed\n"
     ]
    }
   ],
   "source": [
    "with exp.start_logging() as notebook_run:\n",
    "    notebook_run.log(name=\"message\", value=\"Hello from run!\")\n",
    "    print(\"Is it still running?\",notebook_run.get_status())\n",
    "    \n",
    "print(\"Has it completed?\",notebook_run.get_status())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look at submitting a run as a separate Python process. To keep the example simple, we submit the run on local computer. Other targets could include remote VMs and Machine Learning Compute clusters in your Azure ML Workspace.\n",
    "\n",
    "We use *hello.py* script as an example. To perform logging, we need to get a reference to the Run instance from within the scope of the script. We do this using *Run.get_context* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright (c) Microsoft. All rights reserved.\r\n",
      "# Licensed under the MIT license.\r\n",
      "\r\n",
      "from azureml.core import Run\r\n",
      "\r\n",
      "submitted_run = Run.get_context()\r\n",
      "submitted_run.log(name=\"message\", value=\"Hello from run!\")\r\n"
     ]
    }
   ],
   "source": [
    "!more hello.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submitted runs take a snapshot of the *source_directory* to use when executing. You can control which files are available to the run by using an *.amlignore* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing .amlignore\n"
     ]
    }
   ],
   "source": [
    "%%writefile .amlignore\n",
    "# Exclude the outputs directory automatically created by our earlier runs.\n",
    "/outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's submit the run on a local computer. A standard pattern in Azure ML SDK is to create run configuration, and then use *Experiment.submit* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = ScriptRunConfig(source_directory='.', script='hello.py')\n",
    "\n",
    "local_script_run = exp.submit(run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view the status of the run as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>explore-runs</td><td>explore-runs_1605340191_c5915ce9</td><td>azureml.scriptrun</td><td>Preparing</td><td><a href=\"https://ml.azure.com/experiments/explore-runs/runs/explore-runs_1605340191_c5915ce9?wsid=/subscriptions/888519c8-2387-461a-aff3-b31b86e2438e/resourcegroups/aml-quickstarts-126067/workspaces/quick-starts-ws-126067\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: explore-runs,\n",
       "Id: explore-runs_1605340191_c5915ce9,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Preparing)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(local_script_run.get_status())\n",
    "local_script_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submitted runs have additional log files you can inspect using *get_details_with_logs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'runId': 'explore-runs_1605340191_c5915ce9',\n",
       " 'target': 'local',\n",
       " 'status': 'Preparing',\n",
       " 'properties': {'_azureml.ComputeTargetType': 'local',\n",
       "  'ContentSnapshotId': 'e6566ec7-cd07-46ee-b568-f015d4871e2d'},\n",
       " 'inputDatasets': [],\n",
       " 'outputDatasets': [],\n",
       " 'runDefinition': {'script': 'hello.py',\n",
       "  'useAbsolutePath': False,\n",
       "  'arguments': [],\n",
       "  'sourceDirectoryDataStore': None,\n",
       "  'framework': 'Python',\n",
       "  'communicator': 'None',\n",
       "  'target': 'local',\n",
       "  'dataReferences': {},\n",
       "  'data': {},\n",
       "  'outputData': {},\n",
       "  'jobName': None,\n",
       "  'maxRunDurationSeconds': 2592000,\n",
       "  'nodeCount': 1,\n",
       "  'priority': None,\n",
       "  'environment': {'name': 'Experiment explore-runs Environment',\n",
       "   'version': 'Autosave_2020-11-14T07:49:52Z_64c97215',\n",
       "   'python': {'interpreterPath': 'python',\n",
       "    'userManagedDependencies': False,\n",
       "    'condaDependencies': {'channels': ['anaconda', 'conda-forge'],\n",
       "     'dependencies': ['python=3.6.2', {'pip': ['azureml-defaults']}],\n",
       "     'name': 'azureml_da3e97fcb51801118b8e80207f3e01ad'},\n",
       "    'baseCondaEnvironment': None},\n",
       "   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n",
       "   'docker': {'baseImage': 'mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04:20200821.v1',\n",
       "    'platform': {'os': 'Linux', 'architecture': 'amd64'},\n",
       "    'baseDockerfile': None,\n",
       "    'baseImageRegistry': {'address': None, 'username': None, 'password': None},\n",
       "    'enabled': False,\n",
       "    'arguments': []},\n",
       "   'spark': {'repositories': [], 'packages': [], 'precachePackages': True},\n",
       "   'inferencingStackVersion': None},\n",
       "  'history': {'outputCollection': True,\n",
       "   'directoriesToWatch': ['logs'],\n",
       "   'enableMLflowTracking': True,\n",
       "   'snapshotProject': True},\n",
       "  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'parallelTask': {'maxRetriesPerWorker': 0,\n",
       "   'workerCountPerNode': 1,\n",
       "   'terminalExitCodes': None,\n",
       "   'configuration': {}},\n",
       "  'amlCompute': {'name': None,\n",
       "   'vmSize': None,\n",
       "   'retainCluster': False,\n",
       "   'clusterMaxNodeCount': None},\n",
       "  'aiSuperComputer': {'instanceType': None,\n",
       "   'frameworkImage': None,\n",
       "   'imageVersion': None,\n",
       "   'location': None},\n",
       "  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n",
       "  'mpi': {'processCountPerNode': 1},\n",
       "  'hdi': {'yarnDeployMode': 'Cluster'},\n",
       "  'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5},\n",
       "  'exposedPorts': None,\n",
       "  'docker': {'useDocker': False,\n",
       "   'sharedVolumes': True,\n",
       "   'shmSize': '2g',\n",
       "   'arguments': []},\n",
       "  'cmk8sCompute': {'configuration': {}}},\n",
       " 'logFiles': {'azureml-logs/60_control_log.txt': \"[2020-11-14T07:49:56.250529] Using urllib.request Python 3.0 or later\\nStreaming log file azureml-logs/60_control_log.txt\\nRunning: ['/bin/bash', '/tmp/azureml_runs/explore-runs_1605340191_c5915ce9/azureml-environment-setup/conda_env_checker.sh']\\nStarting the daemon thread to refresh tokens in background for process with pid = 10184\\nMaterialized conda environment not found on target: /home/azureuser/.azureml/envs/azureml_da3e97fcb51801118b8e80207f3e01ad\\n\\n\\n[2020-11-14T07:49:56.366451] Logging experiment preparation status in history service.\\nRunning: ['/bin/bash', '/tmp/azureml_runs/explore-runs_1605340191_c5915ce9/azureml-environment-setup/conda_env_builder.sh']\\nRunning: ['conda', '--version']\\nconda 4.9.1\\n\\nCreating conda environment...\\nRunning: ['conda', 'env', 'create', '-p', '/home/azureuser/.azureml/envs/azureml_da3e97fcb51801118b8e80207f3e01ad', '-f', 'azureml-environment-setup/mutated_conda_dependencies.yml']\\nWarning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies.  Conda may not use the correct pip to install your packages, and they may end up in the wrong place.  Please add an explicit pip dependency.  I'm adding one for you, but still nagging you.\\n\"}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_script_run.get_details_with_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use *wait_for_completion* method to block the local execution until remote run is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: explore-runs_1605340191_c5915ce9\n",
      "Web View: https://ml.azure.com/experiments/explore-runs/runs/explore-runs_1605340191_c5915ce9?wsid=/subscriptions/888519c8-2387-461a-aff3-b31b86e2438e/resourcegroups/aml-quickstarts-126067/workspaces/quick-starts-ws-126067\n",
      "\n",
      "Streaming azureml-logs/60_control_log.txt\n",
      "=========================================\n",
      "\n",
      "[2020-11-14T07:49:56.250529] Using urllib.request Python 3.0 or later\n",
      "Streaming log file azureml-logs/60_control_log.txt\n",
      "Running: ['/bin/bash', '/tmp/azureml_runs/explore-runs_1605340191_c5915ce9/azureml-environment-setup/conda_env_checker.sh']\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 10184\n",
      "Materialized conda environment not found on target: /home/azureuser/.azureml/envs/azureml_da3e97fcb51801118b8e80207f3e01ad\n",
      "\n",
      "\n",
      "[2020-11-14T07:49:56.366451] Logging experiment preparation status in history service.\n",
      "Running: ['/bin/bash', '/tmp/azureml_runs/explore-runs_1605340191_c5915ce9/azureml-environment-setup/conda_env_builder.sh']\n",
      "Running: ['conda', '--version']\n",
      "conda 4.9.1\n",
      "\n",
      "Creating conda environment...\n",
      "Running: ['conda', 'env', 'create', '-p', '/home/azureuser/.azureml/envs/azureml_da3e97fcb51801118b8e80207f3e01ad', '-f', 'azureml-environment-setup/mutated_conda_dependencies.yml']\n",
      "Warning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies.  Conda may not use the correct pip to install your packages, and they may end up in the wrong place.  Please add an explicit pip dependency.  I'm adding one for you, but still nagging you.\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.9.1\n",
      "  latest version: 4.9.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "python-3.6.2         | 27.0 MB   | ########## | 100% \n",
      "ca-certificates-2020 | 128 KB    | ########## | 100% \n",
      "readline-7.0         | 387 KB    | ########## | 100% \n",
      "openssl-1.0.2u       | 3.1 MB    | ########## | 100% \n",
      "libffi-3.2.1         | 52 KB     | ########## | 100% \n",
      "libedit-3.1          | 171 KB    | ########## | 100% \n",
      "ncurses-6.0          | 907 KB    | ########## | 100% \n",
      "sqlite-3.23.1        | 1.5 MB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "\n",
      "[2020-11-14T07:51:06.943737] Entering context manager injector.\n",
      "[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError'], invocation=['hello.py'])\n",
      "Script type = None\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 10964\n",
      "Entering Run History Context Manager.\n",
      "Current directory:  /tmp/azureml_runs/explore-runs_1605340191_c5915ce9\n",
      "Preparing to call script [ hello.py ] with arguments: []\n",
      "After variable expansion, calling script [ hello.py ] with arguments: []\n",
      "\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 10964\n",
      "\n",
      "\n",
      "[2020-11-14T07:51:08.628780] The experiment completed successfully. Finalizing run...\n",
      "[2020-11-14T07:51:08.628806] Start FinalizingInRunHistory\n",
      "[2020-11-14T07:51:08.629502] Logging experiment finalizing status in history service.\n",
      "Cleaning up all outstanding Run operations, waiting 300.0 seconds\n",
      "2 items cleaning up...\n",
      "Cleanup took 0.12973856925964355 seconds\n",
      "[2020-11-14T07:51:15.706870] Finished context manager injector.\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: explore-runs_1605340191_c5915ce9\n",
      "Web View: https://ml.azure.com/experiments/explore-runs/runs/explore-runs_1605340191_c5915ce9?wsid=/subscriptions/888519c8-2387-461a-aff3-b31b86e2438e/resourcegroups/aml-quickstarts-126067/workspaces/quick-starts-ws-126067\n",
      "\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "local_script_run.wait_for_completion(show_output=True)\n",
    "print(local_script_run.get_status())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add properties and tags\n",
    "\n",
    "Properties and tags help you organize your runs. You can use them to describe, for example, who authored the run, what the results were, and what machine learning approach was used. And as you'll later learn, properties and tags can be used to query the history of your runs to find the important ones.\n",
    "\n",
    "For example, let's add \"author\" property to the run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_azureml.ComputeTargetType': 'local', 'ContentSnapshotId': 'e6566ec7-cd07-46ee-b568-f015d4871e2d', 'author': 'azureml-user'}\n"
     ]
    }
   ],
   "source": [
    "local_script_run.add_properties({\"author\":\"azureml-user\"})\n",
    "print(local_script_run.get_properties())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Properties are immutable. Once you assign a value it cannot be changed, making them useful as a permanent record for auditing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ServiceException:\n",
      "\tCode: 400\n",
      "\tMessage: (UserError) Cannot modify existing values in Properties\n",
      "\tDetails:\n",
      "\n",
      "\tHeaders: {\n",
      "\t    \"Date\": \"Sat, 14 Nov 2020 07:51:31 GMT\",\n",
      "\t    \"Content-Type\": \"application/json; charset=utf-8\",\n",
      "\t    \"Content-Length\": \"582\",\n",
      "\t    \"Connection\": \"keep-alive\",\n",
      "\t    \"Request-Context\": \"appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d\",\n",
      "\t    \"x-ms-response-type\": \"error\",\n",
      "\t    \"x-ms-client-request-id\": \"876d8b53-0283-4174-a5b2-82fe78fb1805\",\n",
      "\t    \"x-ms-client-session-id\": \"\",\n",
      "\t    \"X-Content-Type-Options\": \"nosniff\",\n",
      "\t    \"x-request-time\": \"0.063\",\n",
      "\t    \"Strict-Transport-Security\": \"max-age=15724800; includeSubDomains; preload\"\n",
      "\t}\n",
      "\tInnerException: {\n",
      "    \"additional_properties\": {},\n",
      "    \"error\": {\n",
      "        \"additional_properties\": {\n",
      "            \"debugInfo\": null\n",
      "        },\n",
      "        \"code\": \"UserError\",\n",
      "        \"severity\": null,\n",
      "        \"message\": \"Cannot modify existing values in Properties\",\n",
      "        \"message_format\": null,\n",
      "        \"message_parameters\": null,\n",
      "        \"reference_code\": null,\n",
      "        \"details_uri\": null,\n",
      "        \"target\": null,\n",
      "        \"details\": [],\n",
      "        \"inner_error\": null\n",
      "    },\n",
      "    \"correlation\": {\n",
      "        \"operation\": \"5d57b6d8390e834589b55deba7f89106\",\n",
      "        \"request\": \"3591eec074813044\"\n",
      "    },\n",
      "    \"environment\": \"southcentralus\",\n",
      "    \"location\": \"southcentralus\",\n",
      "    \"time\": {},\n",
      "    \"component_name\": \"run-history\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    local_script_run.add_properties({\"author\":\"different-user\"})\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tags on the other hand can be changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'quality': 'great run'}\n"
     ]
    }
   ],
   "source": [
    "local_script_run.tag(\"quality\", \"great run\")\n",
    "print(local_script_run.get_tags())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'quality': 'fantastic run'}\n"
     ]
    }
   ],
   "source": [
    "local_script_run.tag(\"quality\", \"fantastic run\")\n",
    "print(local_script_run.get_tags())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add a simple string tag. It appears in the tag dictionary with value of None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'quality': 'fantastic run', 'worth another look': None}\n"
     ]
    }
   ],
   "source": [
    "local_script_run.tag(\"worth another look\")\n",
    "print(local_script_run.get_tags())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query properties and tags\n",
    "\n",
    "You can query runs within an experiment that match specific properties and tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Run(Experiment: explore-runs,\n",
       " Id: explore-runs_1605340191_c5915ce9,\n",
       " Type: azureml.scriptrun,\n",
       " Status: Completed)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(exp.get_runs(properties={\"author\":\"azureml-user\"},tags={\"quality\":\"fantastic run\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Run(Experiment: explore-runs,\n",
       " Id: explore-runs_1605340191_c5915ce9,\n",
       " Type: azureml.scriptrun,\n",
       " Status: Completed)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(exp.get_runs(properties={\"author\":\"azureml-user\"},tags=\"worth another look\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start and query child runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use child runs to group together related runs, for example different hyperparameter tuning iterations.\n",
    "\n",
    "Let's use *hello_with_children* script to create a batch of 5 child runs from within a submitted run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright (c) Microsoft. All rights reserved.\r\n",
      "# Licensed under the MIT license.\r\n",
      "\r\n",
      "from azureml.core import Run\r\n",
      "\r\n",
      "run = Run.get_context()\r\n",
      "\r\n",
      "child_runs = run.create_children(count=5)\r\n",
      "for c, child in enumerate(child_runs):\r\n",
      "    child.log(name=\"Hello from child run \", value=c)\r\n",
      "    child.complete()\r\n"
     ]
    }
   ],
   "source": [
    "!more hello_with_children.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: explore-runs_1605340334_8879f805\n",
      "Web View: https://ml.azure.com/experiments/explore-runs/runs/explore-runs_1605340334_8879f805?wsid=/subscriptions/888519c8-2387-461a-aff3-b31b86e2438e/resourcegroups/aml-quickstarts-126067/workspaces/quick-starts-ws-126067\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "\n",
      "[2020-11-14T07:52:17.169713] Entering context manager injector.\n",
      "[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError'], invocation=['hello_with_children.py'])\n",
      "Script type = None\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 11454\n",
      "Entering Run History Context Manager.\n",
      "Current directory:  /tmp/azureml_runs/explore-runs_1605340334_8879f805\n",
      "Preparing to call script [ hello_with_children.py ] with arguments: []\n",
      "After variable expansion, calling script [ hello_with_children.py ] with arguments: []\n",
      "\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 11454\n",
      "\n",
      "\n",
      "[2020-11-14T07:52:46.672820] The experiment completed successfully. Finalizing run...\n",
      "[2020-11-14T07:52:46.672841] Start FinalizingInRunHistory\n",
      "[2020-11-14T07:52:46.673684] Logging experiment finalizing status in history service.\n",
      "Cleaning up all outstanding Run operations, waiting 300.0 seconds\n",
      "7 items cleaning up...\n",
      "Cleanup took 0.44649362564086914 seconds\n",
      "[2020-11-14T07:52:47.955723] Finished context manager injector.\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: explore-runs_1605340334_8879f805\n",
      "Web View: https://ml.azure.com/experiments/explore-runs/runs/explore-runs_1605340334_8879f805?wsid=/subscriptions/888519c8-2387-461a-aff3-b31b86e2438e/resourcegroups/aml-quickstarts-126067/workspaces/quick-starts-ws-126067\n",
      "\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "run_config = ScriptRunConfig(source_directory='.', script='hello_with_children.py')\n",
    "\n",
    "local_script_run = exp.submit(run_config)\n",
    "local_script_run.wait_for_completion(show_output=True)\n",
    "print(local_script_run.get_status())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can start child runs one by one. Note that this is less efficient than submitting a batch of runs, because each creation results in a network call.\n",
    "\n",
    "Child runs too complete automatically as they move out of scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with exp.start_logging() as parent_run:\n",
    "    for c,count in enumerate(range(5)):\n",
    "        with parent_run.child_run() as child:\n",
    "            child.log(name=\"Hello from child run\", value=c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To query the child runs belonging to specific parent, use *get_children* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Run(Experiment: explore-runs,\n",
       " Id: f078812b-6c3a-4ae9-bc2d-c75adfd635a9,\n",
       " Type: None,\n",
       " Status: Completed),\n",
       " Run(Experiment: explore-runs,\n",
       " Id: a087fe50-0887-481f-a91c-4fbcbeeac12c,\n",
       " Type: None,\n",
       " Status: Completed),\n",
       " Run(Experiment: explore-runs,\n",
       " Id: a143a947-758e-4ccf-bab9-d047323b377b,\n",
       " Type: None,\n",
       " Status: Completed),\n",
       " Run(Experiment: explore-runs,\n",
       " Id: 05ad4402-87a4-4a5e-ba36-7ed04a52155c,\n",
       " Type: None,\n",
       " Status: Completed),\n",
       " Run(Experiment: explore-runs,\n",
       " Id: 1bc6cbd4-292e-40c7-b80d-ed85543c2515,\n",
       " Type: None,\n",
       " Status: Completed)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(parent_run.get_children())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancel or fail runs\n",
    "\n",
    "Sometimes, you realize that the run is not performing as intended, and you want to cancel it instead of waiting for it to complete.\n",
    "\n",
    "As an example, let's create a Python script with a delay in the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright (c) Microsoft. All rights reserved.\r\n",
      "# Licensed under the MIT license.\r\n",
      "\r\n",
      "import time\r\n",
      "\r\n",
      "print(\"Wait for 10 seconds..\")\r\n",
      "time.sleep(10)\r\n",
      "print(\"Done waiting\")\r\n"
     ]
    }
   ],
   "source": [
    "!more hello_with_delay.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use *cancel* method to cancel a run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did the run start? Running\n",
      "Did the run cancel? Canceled\n"
     ]
    }
   ],
   "source": [
    "run_config = ScriptRunConfig(source_directory='.', script='hello_with_delay.py')\n",
    "\n",
    "local_script_run = exp.submit(run_config)\n",
    "print(\"Did the run start?\",local_script_run.get_status())\n",
    "local_script_run.cancel()\n",
    "print(\"Did the run cancel?\",local_script_run.get_status())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also mark an unsuccessful run as failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed\n"
     ]
    }
   ],
   "source": [
    "local_script_run = exp.submit(run_config)\n",
    "local_script_run.fail()\n",
    "print(local_script_run.get_status())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce a run\n",
    "\n",
    "When updating or troubleshooting on a model deployed to production, you sometimes need to revisit the original training run that produced the model. To help you with this, Azure ML service by default creates snapshots of your scripts a the time of run submission:\n",
    "\n",
    "You can use *restore_snapshot* to obtain a zip package of the latest snapshot of the script folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/batch/tasks/shared/LS_root/mounts/clusters/notebook126067/code/Users/odl_user_126067/manage-runs/snapshots/0af23864-c62d-4a8c-ba7d-d27f9e9ee3e8.zip'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_script_run.restore_snapshot(path=\"snapshots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then extract the zip package, examine the code, and submit your run again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    " * To learn more about logging APIs, see [logging API notebook](./logging-api/logging-api.ipynb)\n",
    " * To learn more about remote runs, see [train on AML compute notebook](./train-on-amlcompute/train-on-amlcompute.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "roastala"
   }
  ],
  "categories": [
   "how-to-use-azureml",
   "track-and-monitor-experiments"
  ],
  "category": "training",
  "compute": [
   "Local"
  ],
  "datasets": [
   "None"
  ],
  "deployment": [
   "None"
  ],
  "exclude_from_index": false,
  "framework": [
   "None"
  ],
  "friendly_name": "Managing your training runs",
  "index_order": 2,
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "tags": [
   "None"
  ],
  "task": "Monitor and complete runs"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
